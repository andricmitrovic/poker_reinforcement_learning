{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66beb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dqn_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a19c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, cold_start=False):\n",
    "        self.model_path = \"./models/model.pth\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.learning_rate = 0.0001\n",
    "        self.gamma = 0.95\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = 0.995\n",
    "        self.model = DeepQN(input_size = self.state_size, output_size = self.action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the model weights from the path set in the object.\"\"\"\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Acts according to epsilon greedy policy.\"\"\"\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        prediction = self.model(state)\n",
    "        return torch.argmax(prediction).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self, sample_batch_size):\n",
    "        \"\"\"Learning from experience in memory.\"\"\"\n",
    "        \n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return # Don't learn until we have at least batch size experiences\n",
    "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        \n",
    "        state, action, reward, next_state, done = zip(*sample_batch)\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.float)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        \n",
    "        # Predicted Q values with current state\n",
    "        prediction = self.model(state)\n",
    "        target = prediction.clone()\n",
    "        \n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "            \n",
    "        \n",
    "        # Zero out gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calc loss and do back propagation\n",
    "        loss = self.criterion(target, prediction)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()    \n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.sample_batch_size = 128\n",
    "        self.episodes = 20000\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.agent = Agent(self.state_size, self.action_size)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.agent.load_model()\n",
    "\n",
    "    def run_test(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        state = self.env.reset()\n",
    "\n",
    "        self.env.render()\n",
    "        done = False\n",
    "\n",
    "        index = 0\n",
    "        while not done:\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            action = self.agent.act(state)\n",
    "            next_state, _, done, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            index += 1\n",
    "            self.env.render()\n",
    "        self.env.close()\n",
    "        \n",
    "        print(f'Score: {index+1}')\n",
    "\n",
    "    def run_train(self):\n",
    "        scores = []\n",
    "        avg_score = []\n",
    "        total_score = 0\n",
    "        for index_episode in tqdm(range(self.episodes)):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            done = False\n",
    "            episode_score = 0\n",
    "            while not done:\n",
    "                # Turning off render while training\n",
    "                # self.env.render()\n",
    "\n",
    "                # Choose action\n",
    "                action = self.agent.act(state)\n",
    "\n",
    "                # Act\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Remember experience\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_score += 1\n",
    "\n",
    "\n",
    "            # Console output of learning process\n",
    "            #print(f'Episode {index_episode+1}/{self.episodes} Score: {index + 1}')\n",
    "\n",
    "            # Save cumulative reward in this episode and an avarage reward until now\n",
    "            scores.append(episode_score + 1)\n",
    "            total_score += (episode_score + 1)\n",
    "            avg_score.append(total_score / (index_episode+1))\n",
    "\n",
    "            # Pustamo agenta da uci.\n",
    "            self.agent.learn(self.sample_batch_size)\n",
    "\n",
    "            # Early stopping\n",
    "            if index_episode > 10:\n",
    "                last_10 = scores[-10:]\n",
    "                avg = sum(last_10) / 10\n",
    "                if avg > 490:\n",
    "                    break\n",
    "\n",
    "        \n",
    "        # Save model weights\n",
    "        self.agent.model.save()\n",
    "\n",
    "        # Return training history\n",
    "        return scores, avg_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
