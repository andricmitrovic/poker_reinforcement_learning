{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66beb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dqn_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a19c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c8debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, cold_start=True):\n",
    "        self.model_path = \"./models/model.pth\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.learning_rate = 0.0005\n",
    "        self.gamma = 0.95\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.05\n",
    "        self.exploration_decay = 0.9999\n",
    "        \n",
    "        self.policy_net = DeepQN(input_size = self.state_size, output_size = self.action_size).to(device)\n",
    "        self.target_net = DeepQN(input_size = self.state_size, output_size = self.action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters()) #optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        self.criterion = nn.SmoothL1Loss() # Huber loss\n",
    "        \n",
    "        if not cold_start:\n",
    "            if os.path.isfile(self.model_path):\n",
    "                self.load_model()\n",
    "                self.exploration_rate = self.exploration_min\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the model weights from the path set in the object.\"\"\"\n",
    "        self.policy_net.load_state_dict(torch.load(self.model_path))\n",
    "        self.target_net.load_state_dict(policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Acts according to epsilon greedy policy.\"\"\"\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "            prediction = self.policy_net(state)\n",
    "            action = torch.argmax(prediction).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self, sample_batch_size):\n",
    "        \"\"\"Learning from experience in memory.\"\"\"\n",
    "        \n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return # Don't learn until we have at least batch size experiences\n",
    "        \n",
    "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        state, action, reward, next_state, done = zip(*sample_batch)\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "        action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float).to(device)\n",
    "        \n",
    "        # Predicted Q values with current state\n",
    "        prediction = self.policy_net(state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target = prediction.clone()\n",
    "\n",
    "            for idx in range(len(done)):\n",
    "                Q_new = reward[idx]\n",
    "                if not done[idx]:\n",
    "                    next_state_values = self.target_net(next_state[idx])\n",
    "                    Q_new = reward[idx] + self.gamma * torch.max(next_state_values)\n",
    "\n",
    "                target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "            \n",
    "        \n",
    "        # Zero out gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calc loss and do back propagation\n",
    "        loss = self.criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        # Gradient clipping \n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()    \n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "        print(self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.sample_batch_size = 128\n",
    "        self.episodes = 50000\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.agent = Agent(self.state_size, self.action_size)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.agent.load_model()\n",
    "\n",
    "    def run_test(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        state = self.env.reset()\n",
    "\n",
    "        self.env.render()\n",
    "        done = False\n",
    "\n",
    "        index = 0\n",
    "        while not done:\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            action = self.agent.act(state)\n",
    "            next_state, _, done, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            index += 1\n",
    "            self.env.render()\n",
    "        self.env.close()\n",
    "        \n",
    "        print(f'Score: {index+1}')\n",
    "\n",
    "    def run_train(self):\n",
    "        scores = []\n",
    "        avg_score = []\n",
    "        total_score = 0\n",
    "        for index_episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            done = False\n",
    "            episode_score = 0\n",
    "            while not done:\n",
    "                # Turning off render while training\n",
    "                # self.env.render()\n",
    "\n",
    "                # Choose action\n",
    "                action = self.agent.act(state)\n",
    "\n",
    "                # Act\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Remember experience\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_score += 1\n",
    "                \n",
    "            # Perform optimization\n",
    "            self.agent.learn(self.sample_batch_size)\n",
    "\n",
    "            # # Update the target network, copying all weights and biases in DQN\n",
    "            if index_episode % 10 == 0:\n",
    "                self.agent.target_net.load_state_dict(self.agent.policy_net.state_dict())\n",
    "                \n",
    "            # Console output of learning process\n",
    "            print(f'Episode {index_episode+1}/{self.episodes} Score: {episode_score + 1}')\n",
    "\n",
    "            # Save cumulative reward in this episode and an avarage reward until now\n",
    "            scores.append(episode_score + 1)\n",
    "            total_score += (episode_score + 1)\n",
    "            avg_score.append(total_score / (index_episode+1))\n",
    "\n",
    "            # Pustamo agenta da uci.\n",
    "            self.agent.learn(self.sample_batch_size)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Early stopping\n",
    "            if index_episode > 100:\n",
    "                last_100 = scores[-100:]\n",
    "                avg = sum(last_100) / 100\n",
    "                if avg > 485:\n",
    "                    break\n",
    "\n",
    "        \n",
    "        # Save model weights\n",
    "        self.agent.model.save()\n",
    "\n",
    "        # Return training history\n",
    "        return scores, avg_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
